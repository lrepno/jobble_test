{
  "metadata": {
    "name": "jobble",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\nfrom pyspark.sql.functions import split, udf, explode, posexplode, first, lit, col, concat, stddev, mean, max, abs, array, struct\nfrom pyspark.sql.types import ArrayType, StructType, StructField, IntegerType\nfrom pyspark.sql.window import Window\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\ntest_path \u003d \u0027hdfs:///jobble/test.tsv\u0027\ntest \u003d spark.read.csv(test_path, sep\u003d\u0027\\t\u0027, header\u003dTrue, inferSchema\u003dTrue)\ntest.printSchema()\n\ntrain_path \u003d \u0027hdfs:///jobble/train.tsv\u0027\ntrain \u003d spark.read.csv(train_path, sep\u003d\u0027\\t\u0027, header\u003dTrue, inferSchema\u003dTrue)\ntrain.printSchema()"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\ndef split_features(df):\n    df \u003d df.select(\u0027id_job\u0027, posexplode(split(df[\u0027features\u0027], \u0027,\u0027)))\n    features \u003d set(df.columns) - set(\u0027id_job\u0027)\n    df \u003d df.select([col(x).cast(IntegerType()) for x in features])\n    # names \u003d df.filter(\u0027pos \u003d\u003d 0\u0027).withColumn(\u0027name\u0027, concat(lit(\u0027feature_\u0027), col(\u0027col\u0027), lit(\u0027_\u0027))).drop(\u0027pos\u0027).drop(\u0027col\u0027)\n    # df \u003d df.join(names, df[\u0027id_job\u0027] \u003d\u003d names[\u0027id_job\u0027], how\u003d\u0027inner\u0027).drop(names[\u0027id_job\u0027]).filter(\u0027pos !\u003d 0\u0027).withColumn(\u0027col_name\u0027, concat(col(\u0027name\u0027), col(\u0027pos\u0027)))\n    # df.show()\n    # names.show()\n    df \u003d df.groupBy(\u0027id_job\u0027).pivot(\u0027pos\u0027).agg(first(\u0027col\u0027))\n    return df\n    "
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n\nfeatured_test \u003d split_features(test)\nfeatured_train \u003d split_features(train)\nfeatured_train.printSchema()"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\nfeatures \u003d set(featured_train.columns) - set(\u0027id_job\u0027)\nfeatures_std \u003d featured_train.select([stddev(x).alias(x + \u0027_std\u0027) for x in features])\nfeatures_mean \u003d featured_train.select([mean(x).alias(x + \u0027_mean\u0027) for x in features])\n\nout \u003d featured_test.crossJoin(features_std).crossJoin(features_mean)"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\nfor x in features:\n    out \u003d out.withColumn(x + \u0027_zscore\u0027, (col(x) - col(x + \u0027_mean\u0027))/col(x + \u0027_std\u0027))\nout.show(1)"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\ndef melt(df, id_vars, value_vars, var_name\u003d\"variable\", value_name\u003d\"value\"):\n\n    _vars_and_vals \u003d array(*(\n        struct(lit(c).alias(var_name), col(c).alias(value_name)) \n        for c in value_vars))\n\n    # Add to the DataFrame and explode\n    _tmp \u003d df.withColumn(\"_vars_and_vals\", explode(_vars_and_vals))\n\n    cols \u003d id_vars + [\n            col(\"_vars_and_vals\")[x].alias(x) for x in [var_name, value_name]]\n    return _tmp.select(*cols)\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n\n# out \u003d featured_test.crossJoin(features_std).crossJoin(features_mean)\n\ndf \u003d test.select(\u0027id_job\u0027, posexplode(split(test[\u0027features\u0027], \u0027,\u0027)))\nfeatures \u003d set(df.columns) - set(\u0027id_job\u0027)\n\ndf \u003d df.select([col(x).cast(IntegerType()) for x in features])\ncode_type \u003d df.filter(\u0027pos \u003d\u003d 0\u0027)\n\nwnd \u003d Window().partitionBy(\u0027id_job\u0027)\n\ntmp \u003d df.withColumn(\u0027max_value\u0027, max(\u0027col\u0027).over(wnd)).filter(\u0027max_value \u003d\u003d col\u0027).dropDuplicates(subset\u003d[\u0027id_job\u0027]).select([\u0027id_job\u0027, col(\u0027pos\u0027).alias(\u0027max_feature\u0027), col(\u0027col\u0027).alias(\u0027max_value\u0027)]) #max value is not unique?\n\nfeatures_mean_flat \u003d melt(features_mean, id_vars\u003d[\u0027id_job_mean\u0027], value_vars\u003dset(features_mean.columns)).withColumn(\u0027variable_id\u0027, split(col(\u0027variable\u0027), \u0027_\u0027)[0])\ntmp \u003d tmp.join(features_mean_flat, tmp[\u0027max_feature\u0027] \u003d\u003d features_mean_flat[\u0027variable_id\u0027]).withColumn(\u0027max_feature_abs_mean_diff\u0027, abs(col(\u0027max_value\u0027) - features_mean_flat[\u0027value\u0027])).select([\u0027id_job\u0027, \u0027max_feature\u0027, \u0027max_feature_abs_mean_diff\u0027])\n\n# df.join(tmp, [df[\u0027id_job\u0027] \u003d\u003d tmp[\u0027id_job\u0027], df[\u0027col\u0027] \u003d\u003d tmp[\u0027max_col\u0027]]).count()\n# tmp.show()\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n\nfeatures \u003d set(featured_train.columns) - set(\u0027id_job\u0027)\nfeatures \u003d list(features - set (\u0027id_job\u0027))\n\nto_select \u003d []\nfor x in features:\n    to_select.append(x + \u0027_zscore\u0027)\n\nto_select.append(\u0027max_feature\u0027)\nto_select.append(\u0027max_feature_abs_mean_diff\u0027)\nto_select.append(\u00270\u0027)\n\nout \u003d out.join(tmp, out[\u0027id_job\u0027] \u003d\u003d tmp[\u0027id_job\u0027]).drop(tmp[\u0027id_job\u0027])\n\nfinal \u003d melt(out, id_vars\u003d[\u0027id_job\u0027], value_vars\u003dto_select)\nfinal.cache()"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\nfinal.filter(\u0027variable \u003d\u003d 0\u0027).count()"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\nout.printSchema()"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    }
  ]
}